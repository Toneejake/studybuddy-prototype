{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlARLjDOknppk9hzXNy98P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toneejake/studybuddy-prototype/blob/main/black_hole_sun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27e9a205"
      },
      "source": [
        "**Cell 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcca3a72",
        "outputId": "5b7870f1-aae2-4685-b5f8-e57ff6e5039b"
      },
      "source": [
        "# This cell installs all necessary libraries.\n",
        "# It uses a robust method to check for existing packages before installing.\n",
        "print(\"⏳ Installing dependencies...\")\n",
        "import subprocess, sys, importlib\n",
        "\n",
        "def pip_install_many(args):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args, \"-q\"])\n",
        "\n",
        "try:\n",
        "    import torch, torchvision, torchaudio\n",
        "except Exception:\n",
        "    pip_install_many([\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "\n",
        "for pkg_name, import_name in [\n",
        "    (\"datasets\", \"datasets\"), (\"Pillow\", \"PIL\"), (\"matplotlib\", \"matplotlib\"),\n",
        "    (\"scikit-learn\", \"sklearn\"), (\"tqdm\", \"tqdm\"), (\"gymnasium\", \"gymnasium\"),\n",
        "    (\"stable-baselines3\", \"stable_baselines3\"), (\"sb3-contrib\", \"sb3_contrib\"),\n",
        "    (\"opencv-python-headless\", \"cv2\")\n",
        "]:\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "    except Exception:\n",
        "        pip_install_many([pkg_name])\n",
        "\n",
        "print(\"✅ All dependencies are ready.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing dependencies...\n",
            "✅ All dependencies are ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e695d87"
      },
      "source": [
        "**Cell 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef0a2d07",
        "outputId": "9395c4b1-bd79-4254-b18c-bf8b6c8f49e5"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.maskable.utils import get_action_masks\n",
        "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Core configuration dictionary\n",
        "CFG = {\n",
        "    \"image_size\": 256,\n",
        "    \"obs_fire_ds\": 64,\n",
        "    \"max_agents\": 10,\n",
        "    \"max_steps\": 500,\n",
        "    \"seed\": 42,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(CFG[\"seed\"])\n",
        "torch.manual_seed(CFG[\"seed\"])\n",
        "\n",
        "print(f\"Configuration loaded. Using device: {CFG['device']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded. Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c33a8b8"
      },
      "source": [
        "**Cell 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05a39968",
        "outputId": "6ef13cdc-a2a6-4fd2-d138-60cd41b2bebe"
      },
      "source": [
        "# The U-Net architecture is unchanged from V1.5.\n",
        "# We need to define it so we can load the pre-trained weights.\n",
        "# THIS IS THE CORRECTED VERSION WITH THE ORIGINAL LAYER NAMES.\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def double_conv(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.dconv_down1 = double_conv(3, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.dconv_down4(x)\n",
        "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.dconv_up3(x)\n",
        "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.dconv_up2(x)\n",
        "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        x = self.dconv_up1(x)\n",
        "        out = self.conv_last(x)\n",
        "        return out\n",
        "\n",
        "print(\"✅ U-Net architecture defined with original layer names.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ U-Net architecture defined with original layer names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32de6d22"
      },
      "source": [
        "**Cell 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "c63974cf",
        "outputId": "fd1f57f7-7518-443c-87ab-70556e921332"
      },
      "source": [
        "# This cell loads your pre-trained U-Net model.\n",
        "# It will prompt you to upload the file.\n",
        "\n",
        "perception_model = UNet().to(CFG['device'])\n",
        "MODEL_PATH = \"unet_floorplan_model.pth\"\n",
        "\n",
        "print(f\"Please upload your trained U-Net model file: '{MODEL_PATH}'\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if MODEL_PATH in uploaded:\n",
        "    perception_model.load_state_dict(torch.load(MODEL_PATH, map_location=CFG['device']))\n",
        "    perception_model.eval()\n",
        "    print(f\"\\n✅ Perception AI model '{MODEL_PATH}' loaded successfully!\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"'{MODEL_PATH}' not found. Please upload the correct file.\")\n",
        "\n",
        "# Define the inference function that uses the loaded model\n",
        "def create_grid_from_image(model, pil_image, image_size, device):\n",
        "    model.eval()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)), transforms.ToTensor(),\n",
        "    ])\n",
        "    input_tensor = transform(pil_image.convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    grid_tensor = (torch.sigmoid(output) > 0.5).float().squeeze().cpu()\n",
        "    return grid_tensor.numpy()\n",
        "\n",
        "print(\"Inference function is ready.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your trained U-Net model file: 'unet_floorplan_model.pth'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d7cf3ee7-11c6-476b-ac92-6d7c06ca2561\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d7cf3ee7-11c6-476b-ac92-6d7c06ca2561\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving unet_floorplan_model.pth to unet_floorplan_model.pth\n",
            "\n",
            "✅ Perception AI model 'unet_floorplan_model.pth' loaded successfully!\n",
            "Inference function is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62e0b904"
      },
      "source": [
        "**Cell 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d356897",
        "outputId": "c2489007-bbe4-4533-fb65-cb596770a471"
      },
      "source": [
        "import heapq\n",
        "\n",
        "# These classes are the fundamental building blocks of the simulation.\n",
        "# They are unchanged from our previous discussions.\n",
        "\n",
        "def a_star_search(grid, start, goal, fire_map=None):\n",
        "    def heuristic(a, b): return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
        "    neighbors = [(0,1),(0,-1),(1,0),(-1,0)]; close_set, came_from, gscore = set(), {}, {start: 0}\n",
        "    fscore = {start: heuristic(start, goal)}; oheap = []\n",
        "    heapq.heappush(oheap, (fscore[start], start))\n",
        "    while oheap:\n",
        "        current = heapq.heappop(oheap)[1]\n",
        "        if current == goal:\n",
        "            path = [];\n",
        "            while current in came_from: path.append(current); current = came_from[current]\n",
        "            return path[::-1]\n",
        "        close_set.add(current)\n",
        "        for i, j in neighbors:\n",
        "            neighbor = current[0] + i, current[1] + j; tentative_g_score = gscore[current] + 1\n",
        "            if not (0 <= neighbor[0] < grid.shape[1] and 0 <= neighbor[1] < grid.shape[0]): continue\n",
        "            if grid[neighbor[1]][neighbor[0]] == 1: continue\n",
        "            if fire_map is not None and fire_map[neighbor[1]][neighbor[0]] == 1: continue\n",
        "            if neighbor in close_set and tentative_g_score >= gscore.get(neighbor, 0): continue\n",
        "            if tentative_g_score < gscore.get(neighbor, 0) or neighbor not in [i[1] for i in oheap]:\n",
        "                came_from[neighbor], gscore[neighbor] = current, tentative_g_score\n",
        "                fscore[neighbor] = tentative_g_score + heuristic(neighbor, goal); heapq.heappush(oheap, (fscore[neighbor], neighbor))\n",
        "    return []\n",
        "\n",
        "class FireSimulator:\n",
        "    def __init__(self, grid, p=0.25): self.grid, self.p, self.map = grid, p, np.zeros_like(grid, dtype=float); self.dirs = [(0,1),(0,-1),(1,0),(-1,0)]\n",
        "    def start(self, points):\n",
        "        for y, x in points: self.map[y, x] = 1\n",
        "    def step(self):\n",
        "        new_map = self.map.copy()\n",
        "        for r, c in np.argwhere(self.map == 1):\n",
        "            for dr, dc in self.dirs:\n",
        "                nr, nc = r + dr, c + dc\n",
        "                if 0 <= nr < self.map.shape[0] and 0 <= nc < self.map.shape[1] and self.map[nr, nc] == 0 and self.grid[nr, nc] == 0 and np.random.rand() < self.p: new_map[nr, nc] = 1\n",
        "        self.map = new_map\n",
        "    def reset(self, points=None): self.map.fill(0); self.start(points or [])\n",
        "\n",
        "class Person:\n",
        "    def __init__(self, pos): self.ipos, self.pos, self.path, self.status, self.state = tuple(pos), list(pos), [], 'evacuating', 'CALM'; self.speed, self.trip, self.trip_t = 1.0, 0.0, 0\n",
        "    def update_state(self, fire):\n",
        "        if self.trip_t > 0: return\n",
        "        fires = np.argwhere(fire == 1); min_d = np.min(np.linalg.norm(fires - np.array([self.pos[1], self.pos[0]]), axis=1)) if len(fires) > 0 else float('inf')\n",
        "        if min_d < 25: self.state, self.speed, self.trip = 'PANICKED', 1.5, 0.1\n",
        "        elif min_d < 50: self.state, self.speed, self.trip = 'ALERT', 1.2, 0.0\n",
        "        else: self.state, self.speed, self.trip = 'CALM', 1.0, 0.0\n",
        "    def move(self):\n",
        "        if self.trip_t > 0: self.trip_t -= 1; return\n",
        "        if self.state == 'PANICKED' and np.random.rand() < self.trip: self.trip_t = 5; return\n",
        "        for _ in range(int(round(self.speed))):\n",
        "            if self.path: self.pos = self.path.pop(0)\n",
        "    def check_status(self, fire, exits):\n",
        "        if self.status != 'evacuating': return\n",
        "        if fire[int(self.pos[1]), int(self.pos[0])] == 1: self.status = 'burned'\n",
        "        elif any(np.linalg.norm(np.array(self.pos) - np.array(ex)) < 5 for ex in exits): self.status = 'escaped'\n",
        "    def compute_path(self, grid, goal, fire): self.path = a_star_search(grid, (int(self.pos[0]), int(self.pos[1])), goal, fire)\n",
        "    def reset(self): self.pos, self.path, self.status, self.state = list(self.ipos), [], 'evacuating', 'CALM'; self.speed, self.trip, self.trip_t = 1.0, 0.0, 0\n",
        "\n",
        "print(\"✅ Core simulation logic is ready.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Core simulation logic is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8fea27c"
      },
      "source": [
        "**Cell 6**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae742306",
        "outputId": "aaa70b3f-1048-4205-9a46-ba946082325e"
      },
      "source": [
        "# Cell 6: The V2.1 Hybrid Gymnasium Environment (Fully Spawn-Safe)\n",
        "# This version is designed to work robustly with the 'spawn' multiprocessing method by\n",
        "# handling all asset loading internally within each worker process.\n",
        "\n",
        "class EvacuationEnv_v2_Hybrid(gym.Env):\n",
        "    def __init__(self, hf_dataset_name, max_agents, max_steps, use_perception_net=False, perception_model_path=None, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- SPAWN-SAFE INITIALIZATION ---\n",
        "        # Each worker process will load its own copy of the dataset and model.\n",
        "        self.hf_dataset = load_dataset(hf_dataset_name, split=\"train\")\n",
        "\n",
        "        self.use_perception_net = use_perception_net\n",
        "        if self.use_perception_net:\n",
        "            if perception_model_path is None or device is None:\n",
        "                raise ValueError(\"perception_model_path and device must be provided if use_perception_net is True.\")\n",
        "            # Each worker loads the U-Net model from the provided path.\n",
        "            self.perception_model = UNet().to(device)\n",
        "            self.perception_model.load_state_dict(torch.load(perception_model_path, map_location=device))\n",
        "            self.perception_model.eval()\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.perception_model = None\n",
        "            self.device = None\n",
        "        # ------------------------------------\n",
        "\n",
        "        self.image_size, self.max_agents, self.max_steps = CFG[\"image_size\"], max_agents, max_steps\n",
        "        self.MAX_EXITS = (self.image_size - 2) * 4\n",
        "        self.action_space = spaces.Discrete(self.MAX_EXITS)\n",
        "        self._exit_slot_map = self._create_exit_slot_map()\n",
        "        obs_shape = (CFG[\"obs_fire_ds\"]**2) + (self.max_agents * 3) + 1\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"observation\": spaces.Box(low=0, high=1, shape=(obs_shape,), dtype=np.float32),\n",
        "            \"action_mask\": spaces.Box(low=0, high=1, shape=(self.MAX_EXITS,), dtype=np.int8)\n",
        "        })\n",
        "        self.gt_transform = transforms.Compose([transforms.Resize((self.image_size, self.image_size)), transforms.ToTensor()])\n",
        "\n",
        "    def _create_exit_slot_map(self):\n",
        "        slots, idx, w, h = {}, 0, self.image_size, self.image_size\n",
        "        for x in range(1, w - 1): slots[idx] = (x, 1); idx += 1\n",
        "        for x in range(1, w - 1): slots[idx] = (x, h - 2); idx += 1\n",
        "        for y in range(1, h - 1): slots[idx] = (1, y); idx += 1\n",
        "        for y in range(1, h - 1): slots[idx] = (w - 2, y); idx += 1\n",
        "        return slots\n",
        "\n",
        "    def _get_observation(self):\n",
        "        fire_obs = cv2.resize(self.fire_sim.map.astype(np.float32), (CFG[\"obs_fire_ds\"], CFG[\"obs_fire_ds\"])).flatten()\n",
        "        agent_obs = np.zeros(self.max_agents * 3, dtype=np.float32)\n",
        "        state_map = {'CALM': 0.0, 'ALERT': 0.5, 'PANICKED': 1.0}\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            agent_obs[i*3] = agent.pos[0] / self.image_size; agent_obs[i*3+1] = agent.pos[1] / self.image_size\n",
        "            agent_obs[i*3+2] = state_map.get(agent.state, 0.0)\n",
        "        time_obs = np.array([self.current_step / self.max_steps])\n",
        "        obs = np.concatenate([fire_obs, agent_obs, time_obs]).astype(np.float32)\n",
        "        return {\"observation\": obs, \"action_mask\": self.action_mask}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        random_idx = np.random.randint(0, len(self.hf_dataset))\n",
        "        item = self.hf_dataset[random_idx]\n",
        "        if self.use_perception_net:\n",
        "            self.base_grid = create_grid_from_image(self.perception_model, item['plans'], self.image_size, self.device)\n",
        "        else:\n",
        "            gt_tensor = self.gt_transform(item['walls'].convert(\"L\"))\n",
        "            self.base_grid = (gt_tensor > 0.5).squeeze().numpy().astype(float)\n",
        "\n",
        "        self.exits, self.action_mask = [], np.zeros(self.MAX_EXITS, dtype=np.int8)\n",
        "        for idx, (x, y) in self._exit_slot_map.items():\n",
        "            if self.base_grid[y, x] == 0: self.exits.append((x, y)); self.action_mask[idx] = 1\n",
        "\n",
        "        if not self.exits: # Fallback for maps with no exits\n",
        "            valid_points = np.argwhere(self.base_grid == 0)\n",
        "            if len(valid_points) == 0:\n",
        "                self.action_mask[0] = 1 # Enable one action to prevent a crash\n",
        "                self.exits.append(self._exit_slot_map[0])\n",
        "            else:\n",
        "                y, x = valid_points[0]; self.exits.append((x, y))\n",
        "                closest_idx = min(self._exit_slot_map, key=lambda k: np.hypot(self._exit_slot_map[k][0]-x, self._exit_slot_map[k][1]-y))\n",
        "                self.action_mask[closest_idx] = 1\n",
        "\n",
        "        self.fire_sim = FireSimulator(self.base_grid); self.current_step = 0\n",
        "        valid_spawns = np.argwhere(self.base_grid == 0)\n",
        "        if len(valid_spawns) == 0:\n",
        "            self.agents = [] # No place to spawn agents\n",
        "        else:\n",
        "            fire_y, fire_x = valid_spawns[np.random.choice(len(valid_spawns))]\n",
        "            self.fire_sim.reset(points=[(fire_y, fire_x)])\n",
        "            num_agents = np.random.randint(1, self.max_agents + 1)\n",
        "            spawn_indices = np.random.choice(len(valid_spawns), min(num_agents, len(valid_spawns)), replace=False)\n",
        "            self.agents = [Person(pos=(x, y)) for y, x in valid_spawns[spawn_indices] if self.fire_sim.map[y,x]==0]\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        if not self.action_mask[action]: # Handle rare case where model predicts a masked action\n",
        "            return self._get_observation(), -1.0, False, False, {}\n",
        "        reward = -0.01; target_exit = self._exit_slot_map[action]\n",
        "        self.fire_sim.step()\n",
        "        for agent in self.agents:\n",
        "            if agent.status == 'evacuating':\n",
        "                agent.update_state(self.fire_sim.map)\n",
        "                if not agent.path or self.current_step % 10 == 0: agent.compute_path(self.base_grid, target_exit, self.fire_sim.map)\n",
        "                agent.move(); agent.check_status(self.fire_sim.map, self.exits)\n",
        "                if agent.status == 'escaped': reward += 10\n",
        "                elif agent.status == 'burned': reward -= 10\n",
        "        terminated = all(a.status != 'evacuating' for a in self.agents)\n",
        "        truncated = self.current_step >= self.max_steps\n",
        "        return self._get_observation(), reward, terminated, truncated, {}\n",
        "\n",
        "print(\"✅ V2.1 Hybrid Curriculum Environment 'EvacuationEnv_v2_Hybrid' (Fully Spawn-Safe) is defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ V2.1 Hybrid Curriculum Environment 'EvacuationEnv_v2_Hybrid' (Fully Spawn-Safe) is defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "797f09de"
      },
      "source": [
        "**Cell 7**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0dde913",
        "outputId": "a3e49a0c-a27d-48b9-ce70-603b096ee580"
      },
      "source": [
        "# Cell 7: Environment Factory and Dataset Loader (Final Version)\n",
        "import multiprocessing as mp\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "\n",
        "# This is a critical guard for using 'spawn' or 'forkserver' in notebooks.\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        mp.set_start_method('spawn', force=True)\n",
        "        print(\"✅ Multiprocessing start method set to 'spawn'.\")\n",
        "    except RuntimeError:\n",
        "        pass # Can only be set once\n",
        "\n",
        "# The factory now passes strings (file paths, names) instead of large objects.\n",
        "def make_env_v2_1(use_perception_net: bool = False, n_envs=1):\n",
        "    env_kwargs = {\n",
        "        'hf_dataset_name': \"zimhe/pseudo-floor-plan-12k\",\n",
        "        'max_agents': CFG[\"max_agents\"],\n",
        "        'max_steps': CFG[\"max_steps\"],\n",
        "        'use_perception_net': use_perception_net,\n",
        "        'perception_model_path': MODEL_PATH if use_perception_net else None,\n",
        "        'device': CFG['device']\n",
        "    }\n",
        "\n",
        "    # Use vectorized environments for faster training\n",
        "    return make_vec_env(\n",
        "        EvacuationEnv_v2_Hybrid,\n",
        "        n_envs=n_envs,\n",
        "        vec_env_cls=SubprocVecEnv,\n",
        "        env_kwargs=env_kwargs,\n",
        "        # CORRECTED: Pass start_method inside the vec_env_kwargs dictionary\n",
        "        vec_env_kwargs=dict(start_method='spawn')\n",
        "    )\n",
        "\n",
        "print(\"✅ V2.1 Hybrid environment factory (Final Version) is ready.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Multiprocessing start method set to 'spawn'.\n",
            "✅ V2.1 Hybrid environment factory (Final Version) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ffdb95"
      },
      "source": [
        "**Cell 8**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "4e09fd9e",
        "outputId": "11f696af-6f51-44a0-89c9-90241a7490bd"
      },
      "source": [
        "print(\"--- Running Smoke Test ---\")\n",
        "# Create a single environment using the FAST path (GT masks)\n",
        "test_env = make_env_v2_1(use_perception_net=False, n_envs=1)\n",
        "obs = test_env.reset()\n",
        "print(\"Initial observation shapes are OK.\")\n",
        "\n",
        "# Check if masking works\n",
        "mask = get_action_masks(test_env)\n",
        "print(f\"Action mask shape: {mask.shape}, Number of valid actions: {mask.sum()}\")\n",
        "assert mask.shape[1] == test_env.action_space[0].n, \"Mask shape mismatch!\"\n",
        "\n",
        "# Take a few random (but valid) steps\n",
        "for i in range(5):\n",
        "    valid_actions = np.where(get_action_masks(test_env)[0])[0]\n",
        "    action = [np.random.choice(valid_actions)]\n",
        "    obs, _, _, _ = test_env.step(action)\n",
        "print(\"Stepping through the environment works.\")\n",
        "test_env.close()\n",
        "print(\"✅ Smoke test passed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Smoke Test ---\n",
            "Initial observation shapes are OK.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3653885370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check if masking works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action mask shape: {mask.shape}, Number of valid actions: {mask.sum()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mask shape mismatch!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sb3_contrib/common/maskable/utils.py\u001b[0m in \u001b[0;36mget_action_masks\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPECTED_METHOD_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_wrapper_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPECTED_METHOD_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36menv_method\u001b[0;34m(self, method_name, indices, *method_args, **method_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"env_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_remotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menv_is_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVecEnvIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"got end of file during message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7cfffe0"
      },
      "source": [
        "**Cell 9**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2162c15"
      },
      "source": [
        "# Phase 1: Train a baseline model quickly using the perfect ground-truth data.\n",
        "print(\"--- Phase 1: Starting Baseline Model Training ---\")\n",
        "\n",
        "# Training parameters\n",
        "N_ENVS = 4 # A T4 can handle 4-8 parallel environments\n",
        "TRAIN_STEPS_BASELINE = 50000 # Increase to 200k+ for a production model\n",
        "MODEL_BASELINE_PATH = \"ppo_commander_v2.0_baseline.zip\"\n",
        "\n",
        "# Create the vectorized environments for fast training\n",
        "train_env_baseline = make_env_v2_1(use_perception_net=False, n_envs=N_ENVS)\n",
        "\n",
        "# We use MaskablePPO, designed for this kind of problem\n",
        "model_baseline = MaskablePPO(\n",
        "    \"MultiInputPolicy\",\n",
        "    train_env_baseline,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    verbose=1,\n",
        "    device=CFG['device'],\n",
        "    tensorboard_log=\"./tensorboard_logs/baseline/\"\n",
        ")\n",
        "\n",
        "print(f\"🚀 Training baseline model for {TRAIN_STEPS_BASELINE} steps...\")\n",
        "model_baseline.learn(total_timesteps=TRAIN_STEPS_BASELINE, progress_bar=True)\n",
        "\n",
        "model_baseline.save(MODEL_BASELINE_PATH)\n",
        "train_env_baseline.close()\n",
        "print(f\"\\n✅ Baseline model training complete. Model saved to '{MODEL_BASELINE_PATH}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cf6a72"
      },
      "source": [
        "**Cell 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d5ddcfd"
      },
      "source": [
        "import pandas as pd\n",
        "# Phase 2: Measure how well the baseline model performs on imperfect, U-Net generated maps.\n",
        "print(\"\\n--- Phase 2: Evaluating Domain Gap ---\")\n",
        "\n",
        "if not os.path.exists(MODEL_BASELINE_PATH):\n",
        "    raise FileNotFoundError(\"Baseline model not found. Please run Phase 1 first.\")\n",
        "\n",
        "N_EVAL_EPISODES = 20 # Use more episodes (e.g., 100) for a more stable estimate\n",
        "\n",
        "# Create two evaluation environments: one with GT, one with U-Net\n",
        "eval_env_gt = make_env_v2_1(use_perception_net=False, n_envs=1)\n",
        "eval_env_unet = make_env_v2_1(use_perception_net=True, n_envs=1)\n",
        "\n",
        "# Load the trained baseline model\n",
        "model_to_eval = MaskablePPO.load(MODEL_BASELINE_PATH)\n",
        "\n",
        "print(f\"Running {N_EVAL_EPISODES} evaluation episodes on Ground-Truth maps...\")\n",
        "mean_reward_gt, std_reward_gt = evaluate_policy(model_to_eval, eval_env_gt, n_eval_episodes=N_EVAL_EPISODES, warn=False)\n",
        "\n",
        "print(f\"Running {N_EVAL_EPISODES} evaluation episodes on U-Net maps...\")\n",
        "mean_reward_unet, std_reward_unet = evaluate_policy(model_to_eval, eval_env_unet, n_eval_episodes=N_EVAL_EPISODES, warn=False)\n",
        "\n",
        "# Display results\n",
        "results = {\n",
        "    \"Environment\": [\"Ground-Truth (Ideal)\", \"U-Net Predicted (Real-World)\"],\n",
        "    \"Mean Reward\": [f\"{mean_reward_gt:.2f}\", f\"{mean_reward_unet:.2f}\"],\n",
        "    \"Std Reward\": [f\"{std_reward_gt:.2f}\", f\"{std_reward_unet:.2f}\"]\n",
        "}\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n--- Domain Gap Analysis ---\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "reward_diff = (mean_reward_gt - mean_reward_unet)\n",
        "print(f\"\\nPerformance drop (Domain Gap): {reward_diff:.2f} reward points.\")\n",
        "print(\"A significant drop indicates that fine-tuning is necessary.\")\n",
        "\n",
        "eval_env_gt.close()\n",
        "eval_env_unet.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e41b76"
      },
      "source": [
        "**Cell 11**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b1bc53d"
      },
      "source": [
        "# Phase 3: Close the domain gap by fine-tuning the model on U-Net data.\n",
        "print(\"\\n--- Phase 3: Starting Fine-Tuning ---\")\n",
        "\n",
        "TRAIN_STEPS_FINETUNE = 25000 # Usually fewer steps are needed for fine-tuning\n",
        "LEARNING_RATE_FINETUNE = 3e-5 # Use a lower learning rate!\n",
        "MODEL_FINETUNED_PATH = \"ppo_commander_v2.0_finetuned.zip\"\n",
        "\n",
        "# Create a new environment that uses the SLOW path (U-Net predictions)\n",
        "train_env_finetune = make_env_v2_1(use_perception_net=True, n_envs=N_ENVS)\n",
        "\n",
        "# Load the baseline model to continue its training\n",
        "model_finetune = MaskablePPO.load(\n",
        "    MODEL_BASELINE_PATH,\n",
        "    env=train_env_finetune,\n",
        "    learning_rate=LEARNING_RATE_FINETUNE, # Set the new, lower learning rate\n",
        "    device=CFG['device']\n",
        ")\n",
        "\n",
        "print(f\"🚀 Fine-tuning model for {TRAIN_STEPS_FINETUNE} steps on U-Net data...\")\n",
        "model_finetune.learn(total_timesteps=TRAIN_STEPS_FINETUNE, progress_bar=True, reset_num_timesteps=False)\n",
        "\n",
        "model_finetune.save(MODEL_FINETUNED_PATH)\n",
        "train_env_finetune.close()\n",
        "print(f\"\\n✅ Fine-tuning complete. Final model saved to '{MODEL_FINETUNED_PATH}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 12**"
      ],
      "metadata": {
        "id": "DscM1rbqwG8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "print(\"\\n--- Grand Finale: Testing the Final, Fine-Tuned Model ---\")\n",
        "\n",
        "if not os.path.exists(MODEL_FINETUNED_PATH):\n",
        "    raise FileNotFoundError(\"Fine-tuned model not found. Please run all phases first.\")\n",
        "\n",
        "print(\"Please upload a NEW, UNSEEN floor plan for the final test.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    test_image_path = list(uploaded.keys())[0]\n",
        "    test_image_pil = Image.open(test_image_path)\n",
        "\n",
        "    # 1. Analyze with Perception AI\n",
        "    print(\"Step 1: Analyzing floor plan...\")\n",
        "    simulation_grid = create_grid_from_image(perception_model, test_image_pil, CFG['image_size'], CFG['device'])\n",
        "\n",
        "    # 2. Setup a single test environment with the U-Net's grid\n",
        "    # We must create a temporary, non-randomized environment for this single test\n",
        "    class TestEnv(EvacuationEnv_v2_Hybrid):\n",
        "        def __init__(self, grid):\n",
        "            # Hacky way to bypass the dataset loading for a single grid test\n",
        "            dummy_dataset = [{'plans': test_image_pil, 'walls': test_image_pil}]\n",
        "            super().__init__(dummy_dataset, CFG['max_agents'], CFG['max_steps'])\n",
        "            self.fixed_grid = grid\n",
        "        def reset(self, seed=None, options=None):\n",
        "            self.base_grid = self.fixed_grid # Override with our grid\n",
        "            # Now call the original reset logic, but it will use our fixed grid\n",
        "            return super().reset(seed=seed)\n",
        "\n",
        "    final_test_env = TestEnv(simulation_grid)\n",
        "    model_final = MaskablePPO.load(MODEL_FINETUNED_PATH)\n",
        "    print(\"Step 2: Environment and AI are ready.\")\n",
        "\n",
        "    # 3. Run simulation\n",
        "    print(\"Step 3: Running AI-controlled evacuation...\")\n",
        "    obs, _ = final_test_env.reset()\n",
        "    history = []\n",
        "    while True:\n",
        "        action, _ = model_final.predict(obs, action_masks=obs[\"action_mask\"], deterministic=True)\n",
        "        obs, _, terminated, truncated, _ = final_test_env.step(action[0])\n",
        "        history.append({\n",
        "            'fire': final_test_env.fire_sim.map.copy(),\n",
        "            'agents': [{'p': a.pos, 's': a.status, 'st': a.state, 't': a.trip_t > 0} for a in final_test_env.agents],\n",
        "            'step': final_test_env.current_step\n",
        "        })\n",
        "        if terminated or truncated: break\n",
        "    print(\"✅ Simulation complete.\")\n",
        "\n",
        "    # 4. Animate and display report\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    bg_img = test_image_pil.resize((CFG['image_size'], CFG['image_size']))\n",
        "    def animate(i):\n",
        "        ax.clear(); frame = history[i]\n",
        "        ax.imshow(bg_img, extent=[0, CFG['image_size'], 0, CFG['image_size']])\n",
        "        ax.imshow(frame['fire'], cmap='Reds', alpha=0.5, extent=[0, CFG['image_size'], CFG['image_size'], 0])\n",
        "        for a in frame['agents']:\n",
        "            c={'CALM':'blue','ALERT':'yellow','PANICKED':'orange','escaped':'lime','burned':'black'}.get(a['s'], a['st'])\n",
        "            shape = patches.Ellipse(a['p'],6,3,color=c,alpha=.9) if a['t'] else patches.Circle(a['p'],3,color=c,alpha=.9)\n",
        "            ax.add_patch(shape)\n",
        "        ax.set_title(f\"SafeScape V2.1 Evacuation | Step: {frame['step']}\"); ax.axis('off')\n",
        "    anim = FuncAnimation(fig, animate, frames=len(history), interval=100); plt.close()\n",
        "    display(HTML(anim.to_jshtml()))\n",
        "\n",
        "    # 5. Risk Assessment Dashboard\n",
        "    final_agents = history[-1]['agents']\n",
        "    escaped = sum(1 for a in final_agents if a['s'] == 'escaped')\n",
        "    burned = sum(1 for a in final_agents if a['s'] == 'burned')\n",
        "    total = len(final_agents)\n",
        "    print(\"\\n--- SafeScape V2.1 Risk Assessment ---\")\n",
        "    print(f\"Outcome for '{test_image_path}':\")\n",
        "    print(f\"  - Agents Escaped: {escaped}/{total} ({escaped/total:.1%})\")\n",
        "    print(f\"  - Agents Burned:  {burned}/{total} ({burned/total:.1%})\")"
      ],
      "metadata": {
        "id": "MH9R63XtwFtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}